# ---- Safety-first public demo ----
PUBLIC_DEMO_MODE=1

# Storage
SQLITE_PATH=data/index.sqlite

# Retrieval
EMBEDDINGS_BACKEND=hash          # none | hash | sentence-transformers
EMBEDDING_DIM=512
RETRIEVAL_LEXICAL_LIMIT=40
RETRIEVAL_VECTOR_LIMIT=40
RETRIEVAL_LEXICAL_WEIGHT=0.5
RETRIEVAL_VECTOR_WEIGHT=0.5
# RETRIEVAL_DEBUG_STATS=1        # optional candidate-count + latency diagnostics

# Answering
LLM_PROVIDER=extractive          # extractive | ollama | openai | gemini

# Defense-in-depth (demo)
MAX_QUESTION_CHARS=2000
MAX_TOP_K=8
RATE_LIMIT_ENABLED=1
RATE_LIMIT_WINDOW_S=60
RATE_LIMIT_MAX_REQUESTS=30

# Observability
LOG_LEVEL=INFO
# OTEL_ENABLED=1
# OTEL_TRACES_EXPORTER=auto      # auto | none | otlp | gcp_trace
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318/v1/traces

# Optional OCR (local, open source)
OCR_ENABLED=0
OCR_MAX_PAGES=10
OCR_DPI=200
OCR_LANG=eng

# ---- Local / private (examples) ----
#
# Optional auth for private deployments
# AUTH_MODE=api_key
# API_KEYS_JSON={"reader-key":"reader","editor-key":"editor","admin-key":"admin"}
# Optional tenant-scoped grants:
# API_KEYS_JSON={"reader-a":{"role":"reader","tenants":["tenant-a"]},"admin":{"role":"admin","tenants":"*"}}
#
# Allow interactive ingestion
# ALLOW_UPLOADS=1
# ALLOW_EVAL=1
#
# Local open LLM via Ollama
# LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.1:8b
#
# Better embeddings (downloads a model)
# EMBEDDINGS_BACKEND=sentence-transformers
# EMBEDDINGS_MODEL=all-MiniLM-L6-v2
